{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-17T12:37:01.421645Z",
     "start_time": "2025-02-17T12:37:00.007944Z"
    }
   },
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:37:05.669611Z",
     "start_time": "2025-02-17T12:37:01.474825Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # The CoNLL-2003 dataset for NER\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)"
   ],
   "id": "9e58a89ef6e4f333",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:37:05.785768Z",
     "start_time": "2025-02-17T12:37:05.748523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example = dataset[\"train\"][848]\n",
    "example"
   ],
   "id": "181065dc847ad495",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '848',\n",
       " 'tokens': ['Dean',\n",
       "  'Palmer',\n",
       "  'hit',\n",
       "  'his',\n",
       "  '30th',\n",
       "  'homer',\n",
       "  'for',\n",
       "  'the',\n",
       "  'Rangers',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 22, 38, 29, 16, 21, 15, 12, 23, 7],\n",
       " 'chunk_tags': [11, 12, 21, 11, 12, 12, 13, 11, 12, 0],\n",
       " 'ner_tags': [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:37:05.931552Z",
     "start_time": "2025-02-17T12:37:05.908135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label2id = {\n",
    "      \"O\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-ORG\": 3, \"I-ORG\": 4,\n",
    "      \"B-LOC\": 5, \"I-LOC\": 6, \"B-MISC\": 7, \"I-MISC\": 8\n",
    "}\n",
    "id2label = {index: label for  label, index  in label2id.items()}\n",
    "label2id"
   ],
   "id": "f8a0d321fd7f4d1d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-PER': 1,\n",
       " 'I-PER': 2,\n",
       " 'B-ORG': 3,\n",
       " 'I-ORG': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-LOC': 6,\n",
       " 'B-MISC': 7,\n",
       " 'I-MISC': 8}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:37:06.706918Z",
     "start_time": "2025-02-17T12:37:05.971223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(label2id),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")"
   ],
   "id": "bfceb80f8ab6443d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:37:06.797734Z",
     "start_time": "2025-02-17T12:37:06.764515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_ids = tokenizer(example['tokens'], is_split_into_words=True)['input_ids']\n",
    "print(example['tokens'])\n",
    "tokenizer.convert_ids_to_tokens(token_ids)"
   ],
   "id": "12f79f812e11671",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dean', 'Palmer', 'hit', 'his', '30th', 'homer', 'for', 'the', 'Rangers', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Dean',\n",
       " 'Palmer',\n",
       " 'hit',\n",
       " 'his',\n",
       " '30th',\n",
       " 'home',\n",
       " '##r',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Rangers',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:37:53.810347Z",
     "start_time": "2025-02-17T12:37:53.706672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def align_labels(examples):\n",
    "    # Tokenize the input tokens (words)\n",
    "    token_ids = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,  # Enable truncation if necessary\n",
    "        is_split_into_words=True  # Indicate that input is pre-split into words\n",
    "    )\n",
    "\n",
    "    # Get the NER tags (labels) from the examples\n",
    "    labels = examples[\"ner_tags\"]\n",
    "    updated_labels = []\n",
    "\n",
    "    # Iterate over each example in the batch\n",
    "    for index, label in enumerate(labels):\n",
    "        # Map tokens to their respective words\n",
    "        word_ids = token_ids.word_ids(batch_index=index)\n",
    "        print(word_ids)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        # Iterate over each token's word ID\n",
    "        for word_idx in word_ids:\n",
    "            # If the token corresponds to a new word\n",
    "            if word_idx != previous_word_idx:\n",
    "                previous_word_idx = word_idx\n",
    "                # Assign the label for the word (or -100 for special tokens)\n",
    "                updated_label = -100 if word_idx is None else label[word_idx]\n",
    "                # If the label is B-XXX, change it to I-XXX\n",
    "                if updated_label != -100 and updated_label % 2 == 1:\n",
    "                    updated_label += 1\n",
    "                label_ids.append(updated_label)\n",
    "            else:\n",
    "                # For subword tokens, assign -100 (ignore in loss calculation)\n",
    "                label_ids.append(-100)\n",
    "\n",
    "        # Append the processed labels for this example\n",
    "        updated_labels.append(label_ids)\n",
    "\n",
    "    # Add the updated labels to the tokenized output\n",
    "    token_ids[\"labels\"] = updated_labels\n",
    "    return token_ids\n",
    "\n",
    "# Apply the `align_labels` function to the dataset in batches\n",
    "tokenized = dataset.map(align_labels, batched=True)"
   ],
   "id": "ff495f2c352aca64",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:37:56.873941Z",
     "start_time": "2025-02-17T12:37:54.048942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation\n",
    "\n",
    "seqeval = evaluate.load('seqeval')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=2)\n",
    "\n",
    "    true_predictions = []\n",
    "    ture_labels = []\n",
    "\n",
    "    # Doc level prediction\n",
    "    for pred, label in enumerate(predictions, labels):\n",
    "        for token_pred, token_label in enumerate(pred, label):\n",
    "            if token_label != -100:\n",
    "                true_predictions.append(id2label[token_pred])\n",
    "                ture_labels.append(id2label[token_label])\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=ture_labels)\n",
    "\n",
    "    return {\"f1\": results[\"overall_f1\"]}\n"
   ],
   "id": "28e19ac5054aa28b",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:37:56.913652Z",
     "start_time": "2025-02-17T12:37:56.905703Z"
    }
   },
   "cell_type": "code",
   "source": "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)",
   "id": "e54a51bec4271b42",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:38:00.384579Z",
     "start_time": "2025-02-17T12:37:56.946597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "     \"model\",\n",
    "     learning_rate=2e-5,\n",
    "     per_device_train_batch_size=16,\n",
    "     per_device_eval_batch_size=16,\n",
    "     num_train_epochs=1,\n",
    "     weight_decay=0.01,\n",
    "     save_strategy=\"epoch\",\n",
    "     report_to=\"none\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "      model=model,\n",
    "      args=training_args,\n",
    "      train_dataset=tokenized[\"train\"],\n",
    "      eval_dataset=tokenized[\"test\"],\n",
    "      processing_class=tokenizer,\n",
    "      data_collator=data_collator,\n",
    "      compute_metrics=compute_metrics,\n",
    ")"
   ],
   "id": "7cb2d89d490114ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/0b_xwd412532nmd6152zbw1w0000gp/T/ipykernel_28650/2897590474.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.evaluate()",
   "id": "9f98f23a77b8bb98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save our fine-tuned model\n",
    "trainer.save_model(\"ner_model\")\n",
    "# Run inference on the fine-tuned model\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"ner_model\",\n",
    ")\n",
    "token_classifier(\"My name is Maarten.\")"
   ],
   "id": "a35b3d315407861a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
